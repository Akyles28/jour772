---
title: "pre_lab_05.Rmd"
author: "derek willis/rob wells"
date: "2025-10-05"
output: html_document
---
###Akira Kyles, 10/6/2025
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Points to hit

1.  Review of previous lab questions/problems.
2.  Demonstration of PDF parsing with Tabula

### Task 1: Load libraries

**Task** Run the following code in the gray-colored codeblock below to load the tidyverse and janitor libraries.

To run the code, click the little green play button (left facing arrow) at the top right of the codeblock. In Rmarkdown data notebooks, we write code inside of codeblocks, and explanatory text in the white area outside of it.

```{r}
# turn off sci notation
options(scipen=999)
library(tidyverse)
library(janitor)
```

## PDF Parsing with Tabula

Tabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here's a perfect example: [active voters by county in Maryland](https://www.elections.maryland.gov/press_room/2022_stats/GG22/Eligible%20Active%20Voters%20by%20County%20-%20GG22.pdf).

### Task 1: Download and Install Tabula

**Task**: [Download and install Tabula](https://tabula.technology/). Tabula works much the same way as Open Refine does -- it works in the browser by spinning up a small webserver in your computer. Start it as you would any other desktop application.

You can also find it by accessing <http://127.0.0.1:8080/> in your browser.

We're now going to scrape data from the [active voters by county in Maryland](https://www.elections.maryland.gov/press_room/2022_stats/GG22/Eligible%20Active%20Voters%20by%20County%20-%20GG22.pdf).

**Download this PDF into your pre_lab_05 folder.**

When Tabula opens, browse to find the PDF in your class folder, and then click import. After it imports, click autodetect tables. You'll see red boxes appear around what Tabula believes are the tables. You'll see it does a pretty good job at this.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters.png"))
```

Now click the green "Preview & Export Extracted Data" button on the top right. You should see something like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters2.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. Put the resulting .csv file it in your pre_lab_05 folder. And then we can read it into R:

### Task 2: Load data

**Task** Load the Maryland voters by county data by running the following codeblock.

```{r}
voters_by_county <- read_csv("tabula-Eligible Active Voters by County - GG22.csv")
View(voters_by_county)
```


## More complicated table

Here's a slightly more involved PDF, from [Maryland's 2020 annual report on unintentional drug and alcohol-related intoxication deaths](https://health.maryland.gov/vsa/Documents/Overdose/Annual_2020_Drug_Intox_Report.pdf). Specifically, we're looking at Table 7 on page 67 of the report which lists the number of fentanyl-related deaths by jurisdiction:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_fentanyl_deaths_1.png"))
```

### Task 3: Get it into Tabula

**Task** Save the PDF ( [Maryland's 2020 annual report on unintentional drug and alcohol-related intoxication deaths](https://health.maryland.gov/vsa/Documents/Overdose/Annual_2020_Drug_Intox_Report.pdf)) and import to Tabula.

We just want page 67, Table 7. Save this PDF in your pre_lab_05 folder.

(To save a single page from a PDF in macOS Preview, **enable the thumbnail sidebar (View \> Thumbnails), then drag the desired page thumbnail to your desktop or a new folder**; this creates a new PDF containing only that page.)

Now let's repeat the steps we did to import the PDF into Tabula. It should look like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_fentanyl_deaths_2.png"))
```

### Task 4: Draw a Box

**Task** Using your cursor, draw a box across the table so it looks like the image below.

Let's draw a box around what we want, but there's a catch: the headers aren't on a single line. If you draw your box around the whole table and preview, you'll see that there's a problem. To fix that, we'll need to limit our box to just the data. Using your cursor, click and drag a box across the table so it looks like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_fentanyl_deaths_3.png"))
```

Now you can hit the green "Preview & Export Extracted Data" button on the top right. Using the "Stream" method, you should see something very like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_fentanyl_deaths_4.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. And then we can read it into R and clean up the column names and some other things:

### Task 5: Export the CSV file

**Task** Export the CSV file to your pre_lab_05 folder and read it into R by running the following codeblock. What problems do you see with the data? **Answer**

```{r}
fentanyl_deaths <- read_csv("tabula-Annual_2020_Drug_Intox_Report.csv") |> clean_names()

fentanyl_deaths
```

## Cleaning up the data in R

The good news is that we have data we don't have to retype. The bad news is, we have a few things to fix, starting with the fact that the headers shouldn't be headers. Let's start by re-importing it and specifying that the first row doesn't have column headers:

### Task 6: Load the data without headers

We need actual headers. Let's add those using `rename()`, keeping in mind that the new name comes *first*.

### Task 7: Change the headers to real names

**Task** Run the codeblock below. What else do we need to do? **Answer**

```{r}
fentanyl_deaths <- read_csv("tabula-Annual_2020_Drug_Intox_Report.csv", col_names = FALSE) |> 
  clean_names() |> 
  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, 
         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12)

fentanyl_deaths
```

We could stop here, but we need to remove periods in the jurisdiction column - it will make filtering easier. Let's use `str_replace_all()` to do that:

### Task 8: Replace the periods

**Task** Run the codeblock below.

```{r}
fentanyl_deaths <- read_csv("tabula-Annual_2020_Drug_Intox_Report.csv", col_names = FALSE) |> 
  clean_names() |> 
  rename(jurisdiction = x1, deaths_2011 = x2, deaths_2012 = x3, deaths_2013 = x4, deaths_2014 = x5, deaths_2015 = x6, deaths_2016 = x7, deaths_2017 = x8, 
         deaths_2018 = x9, deaths_2019 = x10, deaths_2020 = x11, deaths_total = x12) |> 
  mutate(jurisdiction = str_squish(str_replace_all(jurisdiction,'\\.','')))

fentanyl_deaths
```

There are a few important things to explain here:

1.  Because we're replacing a literal period (.), we need to make sure that R knows that. Hence the '\\.' Why? Because '.' is a valid expression meaning "any character", so if we didn't have the backslashes the above code would make the entire column blank (try it!)
2.  The `str_squish` function cleans up any excess spaces, at the beginning, middle or end of a character column. If we then use filter, we can do so with confidence.
3.  I put "deaths\_" in front of each yearly column because R likes it when columns don't begin with a number. You can have a column called `2011`, but you literally have to use the backticks (`2011`) to refer to it in code.

All things considered, that was pretty easy. 
Many - most? - electronic PDFs aren't so easy to parse. Sometimes you'll need to open the exported CSV file and clean things up before importing into R. Other times you'll be able to do that cleaning in R itself.



# Using PDF tools
The pdftools package (https://rpubs.com/kbodwin/pdftools) does a good job importing and scraping PDFs straight into the program. I use it a lot for simpler PDFs.

One issue: the great things Tabula does behind the scenes have to be replicated in our code, as we will see below.



```{r}
#install.packaages("pdftools")
library(pdftools)

#pdf_text reads the text from a PDF file.
text <- pdf_text("Eligible Active Voters by County - GG22.pdf")

# splits each page by newlines into separate lines
#unlist() flattens the data for easy processing into a table

text_data <- unlist(strsplit(text, "\n"))
```

With Tabula, that Autodetect Tables button is pretty powerful. We have to do that with code here. This code says extract the table that begins on line 7 and stop at line 33.

```{r}
extracted_lines <- text_data[7:33] 
```

**Task**: Write a brief answer describing the difference between text_data and extracted_lines?

```{r}
# This code parses the data using read.table and imports headers\

final_df <- read.table(
  text = extracted_lines,
  header = TRUE,
  stringsAsFactors = FALSE,
  fill = TRUE, 
  row.names = NULL
) |> 
   mutate(across(DEM:TOTAL, ~as.numeric(gsub(",", "", .))))
```


### More complicated way using base R. 
I am only showing this to have you appreciate the power of that across command.
```{r}
df <- read.table(
  text = extracted_lines,
  header = TRUE,
  stringsAsFactors = FALSE,
  fill = TRUE, 
  row.names = NULL
)
# Remove commas and convert numeric columns to numeric
numeric_cols <- c("DEM", "REP", "BAR", "GRN", "LIB", "WCP", "OTH", "UNA", "TOTAL")
df[numeric_cols] <- lapply(df[numeric_cols], function(x) as.numeric(gsub(",", "", x)))

```
