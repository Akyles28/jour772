---
title: "lab_05"
author: "Derek Willis"
date: "2025-01-16"
output: html_document
---
###Akira Kyles, 10/8/2025

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

-   Tabula

## Load tidyverse, janitor and lubidate libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse, plus any other packages you will need to clean data and work with dates.
library(tidyverse)
library(janitor)
library(lubridate)
```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore
County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link).

Examine the document:

-   DAT_FORMAT is date of the incident

-   DAT_FORMACAATSE_NBR has the date and time of the incident

-   CASE_NBR is the case number

-   EVTYP is event_type

-   LOC is the location

Download it to your labs folder. You will extract the tables within it,
export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Launch Tabula (you may then need to go to <http://127.0.0.1:8080/> in
your browser). Click the "Browse" button and find the PDF file and click
"open", and then click the "Import button" in Tabula.

This PDF has a single table spread over multiple pages to extract. We're
going to make a single dataframe from this table, exporting it to a CSV
file that you will load into R. In Tabula, highlight the table and click
the "Preview & Export Extracted Data" button.

**HINT: You may want to play with including or excluding the column
headers.**

**YOU MUST HAVE FIVE COLUMNS OF DATA FOR EXPORT.**

The final entry should be 02/06/23 at 5651 BRAXFIELD RD

Save the CSV (it should be called
`tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to
your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers.

You can choose to include the headers from the PDF in your exported
CSV files OR to exclude them and add them when importing. 
`read_csv` allows us to do this ([and
more](https://readr.tidyverse.org/reference/read_delim.html)).
Note the col_names entry: col_names
Either TRUE, FALSE or a character vector of column names.
If TRUE, the first row of the input will be used as the column names, and will not be included in the data frame. If FALSE, column names will be generated automatically: X1, X2, X3 etc.

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used
for analysis. By "clean" I mean the column headers should not contain
spaces and they should have meaningful names, not "x1" or something
similar. How you do that is up to you, but you can use select() with or
without the minus sign to include or exclude certain columns. You also
can use the `rename` function to, well, rename columns. Importantly,
you'll need to ensure that any columns containing a date actually have a
date datatype. `lubridate` can help with this.

```{r}
baltimore_overdoses <- read_csv("tabula-Baltimore County; Carey, Samantha log OD.csv", col_names = FALSE) |>  
  clean_names() |> 
  rename(od_dates = x1, od_time = x2, case_number = x3, ev_type = x4, od_location = x5)

baltimore_overdoses
```
```{r}
bmore_ods <- baltimore_overdoses |> mutate(od_dates=mdy(od_dates))
```
```{r}
head(bmore_ods)
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each
date. 

A1.

```{r}
bmore_ods |> 
  group_by(od_dates) |> 
  summarise(count=n()) |> 
  arrange(desc(count))
```

Q1a: Write down this answer: Which date in 2022 had the most overdose calls, and how many?
**Q1a answer**:
### July 14 and October 4 had the most overdose calls at 23 calls. 

Q1b: Write down this answer: Look at the total number of rows in your result from Q1. How many dates are represented in this data? **Q1b answer**:
### There are 366 dates represented in this data. 

Q2. You want to see if overdose calls fall on a particular day of the week.

**Write code for the following:**
Add a column to your dataframe that shows the day of the week represented by each date. 

You can do this using lubridate. Look up how you would do this. https://lubridate.tidyverse.org/

Then write code to calculate:
1) the number of calls for each day of the week
2) and add a column that calculates the percentage of all calls on each
day of the week.

Your result will have the day of the week, total
number of calls and the percentage of calls on that day out of the total
number of all calls. 


```{r}
bmore_ods <- bmore_ods |> 
  mutate(weekday = wday(
  od_dates,
  label = TRUE,
  abbr = TRUE,
  week_start = getOption("lubridate.week.start", 7),
  locale = Sys.getlocale("LC_TIME")
)) 

weekly_calls <- bmore_ods |> 
  group_by(weekday)|> 
  summarise(count = n()) |> 
  mutate(percentage = count / sum(count) * 100) |> 
  arrange(desc(count))
   
```


**Q2a. Write an answer describing your findings.**
###Just as I figured, weekend days have the highest percentage of call rates. It also made sense for Friday to have the third highest rate but I was surprised that Tuesday was the fourth highest. 

Q3. Now let's look at locations. 

**Write code for the following:**
Which locations have the most calls?

A3.
###4540 Silver Spring Road has the most calls at 36 calls. 

```{r}
baltimore_overdoses |> 
  group_by(od_location) |>
  summarise(count=n()) |> 
  arrange(desc(count))

```

**Q3a. Write an answer to the following:**
How would you describe them? Search on the Web for more information. Is there anything about the structure of the original data that might make you less confident in the counts by location or date?
### The Silver Hill Road address seems to be a regular residential home. I do find it odd that over 30 calls were made to this one house. It makes me less confident that the data is correct because it doesn't seem all that plausible to have the highest amount of calls centered at this one residential home. 


**Q4.  Write an answer to the following:** What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4.
### I think a great story idea from this data set is to examine the areas with the highest call volume to see why that area is more prone to drug users; what police are doing to monitor and deter drug-related crimes in that area; and compare this data set with a crime data set to see if the areas are in high crime places. 

### I think with data like this, it would also be great to accompany it with an interactive map to display the geographical data discovered in the reporting. 
